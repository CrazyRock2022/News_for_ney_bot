#!/usr/bin/env python3
import os
import sys
import time
import json
import signal
import sqlite3
import feedparser
import aiohttp
import asyncio
import logging
import hashlib
from datetime import datetime, timedelta
from contextlib import contextmanager
from aiogram import Bot, Dispatcher, F, types
from aiogram.filters import Command, CommandStart
from aiogram.fsm.context import FSMContext
from aiogram.fsm.state import State, StatesGroup
from aiogram.fsm.storage.memory import MemoryStorage
from aiogram.client.default import DefaultBotProperties
from aiogram.enums import ParseMode
from apscheduler.schedulers.asyncio import AsyncIOScheduler
from tenacity import retry, stop_after_attempt, wait_exponential

# –ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
from dotenv import load_dotenv
load_dotenv()

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
API_TOKEN = os.getenv("BOT_TOKEN")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
openai.api_key = OPENAI_API_KEY

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler("newsbot.log", encoding="utf-8"),
        logging.StreamHandler()
    ]
)

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–æ—Ç–∞
bot = Bot(
    token=API_TOKEN,
    default=DefaultBotProperties(parse_mode=ParseMode.HTML)
)
dp = Dispatcher(storage=MemoryStorage())
scheduler = AsyncIOScheduler()

# –°–æ—Å—Ç–æ—è–Ω–∏—è FSM
class UserState(StatesGroup):
    ADDING_SOURCE = State()
    AWAITING_PROMPT = State()

# –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ –ø–∞—Ä—Å–µ—Ä—ã
def parse_cryptopanic(entry):
    currencies = ", ".join([c["code"] for c in entry.get("currencies", [])])
    return {
        "title": entry.get("title", "–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞"),
        "link": entry.get("link", "#"),
        "content": f"{entry.get('description', '')}\n–ö—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç—ã: {currencies}"
    }

SOURCE_PARSERS = {
    "cryptopanic.com": parse_cryptopanic,
}

# –†–∞–±–æ—Ç–∞ —Å –±–∞–∑–æ–π –¥–∞–Ω–Ω—ã—Ö
@contextmanager
def db_connection():
    conn = sqlite3.connect("newsbot.db")
    conn.row_factory = sqlite3.Row
    try:
        yield conn
    finally:
        conn.close()

def init_db():
    with db_connection() as conn:
        conn.execute("""
        CREATE TABLE IF NOT EXISTS sources (
            url TEXT PRIMARY KEY,
            added_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )""")
        conn.execute("""
        CREATE TABLE IF NOT EXISTS sent_links (
            link TEXT PRIMARY KEY,
            sent_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )""")
        conn.execute("""
        CREATE TABLE IF NOT EXISTS users (
            user_id INTEGER PRIMARY KEY,
            registered_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )""")

# OpenAI –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è
@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=10))
async def analyze_with_gpt(text: str, prompt: str) -> str:
    try:
        response = await openai.ChatCompletion.acreate(
            model="gpt-4-turbo",
            messages=[{
                "role": "user",
                "content": f"{prompt}\n\n{text[:3000]}\n–û—Ç–≤–µ—Ç:"
            }],
            max_tokens=50,
            temperature=0.3
        )
        return response.choices[0].message.content.strip().lower()
    except Exception as e:
        logging.error(f"OpenAI error: {str(e)}")
        return "–Ω–µ—Ç"

# –û–±—Ä–∞–±–æ—Ç–∫–∞ RSS-–ª–µ–Ω—Ç
async def fetch_feed(url: str):
    try:
        async with aiohttp.ClientSession() as session:
            async with session.get(url) as response:
                if response.status == 200:
                    return feedparser.parse(await response.text())
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Ñ–∏–¥–∞ {url}: {str(e)}")
    return None

async def process_entries(feed, url):
    source_name = feed.feed.get("title", url)
    parser = SOURCE_PARSERS.get(url.split("//")[-1].split("/")[0], None)
    
    with db_connection() as conn:
        sent_links = {row["link"] for row in conn.execute("SELECT link FROM sent_links")}
    
    relevant = []
    stats = {"total": 0, "relevant": 0}
    
    for entry in feed.entries[:50]:
        if parser:
            data = parser(entry)
        else:
            data = {
                "title": entry.get("title", ""),
                "link": entry.get("link", ""),
                "content": entry.get("description", "")
            }
        
        if not data["link"] or data["link"] in sent_links:
            continue
        
        stats["total"] += 1
        analysis_text = f"{data['title']}\n{data['content']}"
        answer = await analyze_with_gpt(analysis_text, DEFAULT_PROMPT)
        
        if "–¥–∞" in answer:
            relevant.append((data["title"], data["link"]))
            stats["relevant"] += 1
            with db_connection() as conn:
                conn.execute("INSERT INTO sent_links (link) VALUES (?)", (data["link"],))
    
    return source_name, stats, relevant

# –û—Å–Ω–æ–≤–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç—á–∏–∫ –Ω–æ–≤–æ—Å—Ç–µ–π
async def fetch_news():
    relevant = []
    stats = {}
    
    with db_connection() as conn:
        sources = [row["url"] for row in conn.execute("SELECT url FROM sources")]
    
    for url in sources:
        try:
            feed = await fetch_feed(url)
            if not feed or not feed.entries:
                continue
            
            source_name, source_stats, source_relevant = await process_entries(feed, url)
            stats[source_name] = source_stats
            relevant.extend(source_relevant)
        
        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {url}: {str(e)}")
    
    return relevant, stats

# –•–µ–Ω–¥–ª–µ—Ä—ã –∫–æ–º–∞–Ω–¥
@dp.message(CommandStart())
async def cmd_start(message: types.Message):
    with db_connection() as conn:
        conn.execute("INSERT OR IGNORE INTO users (user_id) VALUES (?)", (message.from_user.id,))
    
    await message.answer(
        "üì∞ –ë–æ—Ç –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –Ω–æ–≤–æ—Å—Ç–µ–π A7A5\n\n"
        "–î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã:\n"
        "/digest - –ü–æ–ª—É—á–∏—Ç—å —Å–≤–µ–∂–∏–µ –Ω–æ–≤–æ—Å—Ç–∏\n"
        "/add_source - –î–æ–±–∞–≤–∏—Ç—å RSS-–∏—Å—Ç–æ—á–Ω–∏–∫\n"
        "/help - –°–ø—Ä–∞–≤–∫–∞",
        reply_markup=types.InlineKeyboardMarkup(inline_keyboard=[
            [types.InlineKeyboardButton(text="üÜò –ü–æ–º–æ—â—å", callback_data="help")]
        )
    )

@dp.callback_query(F.data == "help")
async def show_help(callback: types.CallbackQuery):
    await callback.message.edit_text(
        "‚ÑπÔ∏è –°–ø—Ä–∞–≤–∫–∞ –ø–æ –∫–æ–º–∞–Ω–¥–∞–º:\n\n"
        "/digest - –ó–∞–ø—É—Å—Ç–∏—Ç—å –∞–Ω–∞–ª–∏–∑ –Ω–æ–≤–æ—Å—Ç–µ–π\n"
        "/add_source [url] - –î–æ–±–∞–≤–∏—Ç—å RSS-–ª–µ–Ω—Ç—É\n"
        "/help - –ü–æ–∫–∞–∑–∞—Ç—å —ç—Ç–æ —Å–æ–æ–±—â–µ–Ω–∏–µ"
    )
    await callback.answer()

@dp.message(Command("digest"))
async def cmd_digest(message: types.Message, state: FSMContext):
    await message.answer("‚è≥ –ê–Ω–∞–ª–∏–∑–∏—Ä—É—é –Ω–æ–≤–æ—Å—Ç–∏...")
    
    try:
        relevant, stats = await fetch_news()
        report = ["üìä –û—Ç—á—ë—Ç –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º:"]
        
        for source, data in stats.items():
            report.append(f"‚ñ™Ô∏è {source}: {data['relevant']}/{data['total']}")
        
        if relevant:
            await state.update_data(relevant_news=relevant)
            await message.answer(
                "\n".join(report),
                reply_markup=types.InlineKeyboardMarkup(inline_keyboard=[
                    [types.InlineKeyboardButton(
                        text=f"üì© –ü–æ–ª—É—á–∏—Ç—å –Ω–æ–≤–æ—Å—Ç–∏ ({len(relevant)})", 
                        callback_data="get_news")]
                ])
            )
        else:
            await message.answer("\n".join(report) + "\n\n‚ùå –†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
    
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ digest: {str(e)}")
        await message.answer("‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –Ω–æ–≤–æ—Å—Ç–µ–π")

@dp.message(Command("add_source"))
async def cmd_add_source(message: types.Message, state: FSMContext):
    args = message.text.split()
    if len(args) < 2:
        await message.answer("‚ùå –£–∫–∞–∂–∏—Ç–µ URL RSS-–ª–µ–Ω—Ç—ã\n–ü—Ä–∏–º–µ—Ä: /add_source https://example.com/rss")
        return
    
    url = args[1].strip()
    if not url.startswith("http"):
        await message.answer("‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç URL")
        return
    
    try:
        with db_connection() as conn:
            conn.execute("INSERT INTO sources (url) VALUES (?)", (url,))
        await message.answer(f"‚úÖ –ò—Å—Ç–æ—á–Ω–∏–∫ –¥–æ–±–∞–≤–ª–µ–Ω: {url}")
    except sqlite3.IntegrityError:
        await message.answer("‚ö†Ô∏è –≠—Ç–æ—Ç –∏—Å—Ç–æ—á–Ω–∏–∫ —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")

@dp.callback_query(F.data == "get_news")
async def send_news(callback: types.CallbackQuery, state: FSMContext):
    data = await state.get_data()
    news = data.get("relevant_news", [])
    
    for title, link in news:
        await callback.message.answer(f"<b>{title}</b>\n{link}")
    
    await callback.message.edit_reply_markup(reply_markup=None)
    await callback.answer()

# –ï–∂–µ–¥–Ω–µ–≤–Ω–∞—è —Ä–∞—Å—Å—ã–ª–∫–∞
async def daily_digest():
    logging.info("–ó–∞–ø—É—Å–∫ –µ–∂–µ–¥–Ω–µ–≤–Ω–æ–π —Ä–∞—Å—Å—ã–ª–∫–∏...")
    relevant, _ = await fetch_news()
    
    if not relevant:
        logging.info("–ù–µ—Ç –Ω–æ–≤–æ—Å—Ç–µ–π –¥–ª—è —Ä–∞—Å—Å—ã–ª–∫–∏")
        return
    
    with db_connection() as conn:
        users = [row["user_id"] for row in conn.execute("SELECT user_id FROM users")]
    
    for user_id in users:
        try:
            for title, link in relevant[:5]:
                await bot.send_message(user_id, f"<b>{title}</b>\n{link}")
                await asyncio.sleep(1)
        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ –¥–ª—è {user_id}: {str(e)}")

# –ó–∞–ø—É—Å–∫ –∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ
async def on_shutdown():
    logging.info("–ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã...")
    await bot.close()

async def main():
    init_db()
    scheduler.add_job(daily_digest, "cron", hour=11)
    scheduler.start()
    
    await bot.delete_webhook(drop_pending_updates=True)
    await dp.start_polling(bot)

if __name__ == "__main__":
    signal.signal(signal.SIGTERM, lambda s, f: asyncio.run(on_shutdown()))
    asyncio.run(main())